{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络和机器学习之广告预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例说明：Advertising（广告预测），使用全连接神经网络层。\n",
    "\n",
    "案例选择了keras框架，需要先安装keras和tensorflow。虚谷号教育版已经预装必要的库，可以直接使用。\n",
    "\n",
    "本案例已经提供了训练好的模型，放在`model`文件夹中，文件名称为：`1-model-vv.h5`。如果想直接测试模型，请跳到“导入模型”或者“应用模型”环节，输入数据开始识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.环境搭建\n",
    "\n",
    "下面是安装命令：\n",
    "\n",
    "pip install keras\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow\n",
    "\n",
    "建议选择清华源，速度将快很多。参考命令如下：\n",
    "\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据说明\n",
    "\n",
    "企业为了提高产品销售额，往往会在各种媒体投入资金展示自己的广告，常见的媒体有电视、广播以及报纸。作为决策者，需要确定在不同媒体的最佳广告投入，以期待最好的产品收益。为了寻求各媒体广告投入与收益之间的关系，还收集到了之前两百个不同产品在各个媒体中的广告投入资金与最终的销售业绩数据，这些数据被制成表格保存在“Advertising.csv”文件中。\n",
    "\n",
    "数据总共有5列，第1列表示数据的行号，没有列名；第2列到第4列分别是在电视、广播、报纸上的广告投入，最后一列则是对应的销售额。我们让机器学习这一组数据，然后再输入新的数据，让机器来预测可能的销售额。\n",
    "\n",
    "开始导入数据集吧，数据文件在`data`文件夹中。\n",
    "\n",
    "`Advertising.csv`文件是以纯文本形式存储表格数据（数字和文本）,文件一共201行，第一行为列名。文本内容如下：\n",
    "\n",
    ",TV,radio,newspaper,sales\n",
    "\n",
    "1,230.1,37.8,69.2,22.1\n",
    "\n",
    "2,44.5,39.3,45.1,10.4\n",
    "\n",
    "3,17.2,45.9,69.3,9.3\n",
    "\n",
    "……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('./data/Advertising.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用pandas的read_csv功能，读入csv文件中的数据。data是pandas中的DataFrame对象，是一个二维数组。head 和 tail 方法可以分别查看最前面几行和最后面几行的数据（默认为5）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     TV  radio  newspaper  sales\n",
       "0           1  230.1   37.8       69.2   22.1\n",
       "1           2   44.5   39.3       45.1   10.4\n",
       "2           3   17.2   45.9       69.3    9.3\n",
       "3           4  151.5   41.3       58.5   18.5\n",
       "4           5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>38.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>94.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>177.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>283.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.2</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>232.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     TV  radio  newspaper  sales\n",
       "195         196   38.2    3.7       13.8    7.6\n",
       "196         197   94.2    4.9        8.1    9.7\n",
       "197         198  177.0    9.3        6.4   12.8\n",
       "198         199  283.6   42.0       66.2   25.5\n",
       "199         200  232.1    8.6        8.7   13.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用pandas的iloc，对数据进行切片。`data.iloc[: , 1:-1]`表示x等于，y=data.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[: , 1:-1]\n",
    "y=data.iloc[: , -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，x与y的形状分别是(200,3)和(200,1)，即x具有200行、3列，y具有200行、1列。其中x是输入的数据(电视、广播、报纸上的广告投入)，y是输出的结果(销售额)。可以输出其中某一行看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.5  41.3  58.5]\n",
      "18.5\n"
     ]
    }
   ],
   "source": [
    "print(x.values[3])\n",
    "print(y.values[3])  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "为了让模型可以更好的用于对未知数据的预测，一般会把已有的数据分割成三个数据集，分别是训练集、验证集、测试集。\n",
    "\n",
    "对于这里的200组数据，可以将其先后顺序随机打乱，然后取前160组作为训练集数据，之后的20组为验证集数据，最后的20组为测试集数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sample(frac=1).reset_index(drop=True)   #打乱数据的先后顺序\n",
    "x=data.iloc[:,1:-1]\n",
    "y=data.iloc[:,-1]\n",
    "x_train,y_train=x[:160],y[:160]\n",
    "x_val,y_val=x[160:180],y[160:180]\n",
    "x_test,y_test=x[180:],y[180:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了调用方便，直接导入了keras的layers子集。keras支持建立序惯模型与函数式模型，在一般情况下，建立一个序贯模型就可以了。接着，为模型添加层，keras支持很多类型的神经网络层，这里使用add方法添加2个全连接神经网络层（Dense层）。\n",
    "\n",
    "第一层通过input_dim参数指定接收输入数据的维度为3（电视、广播、报纸3列），units=32表示将这个3维数据全连接到32个神经元，并通过relu激活函数进行激活，当然，这一层中的神经元个数并不一定需要设置为32个，但是较多的神经元个数使得模型具有更强的拟合能力；从第二层开始，输入数据维度默认为前一层的输出维度，因此不再需要指定输入数据的维度，只需要指定神经元个数即可，在上述代码中，第一层的32维输出再次全连接到第二层的1个神经元中，最后这1个神经元的输出就是模型的预测结果了。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(layers.Dense(units=32,input_dim=3,activation='relu'))\n",
    "model.add(layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好模型的层之后，需要对模型进行编译，同时指定训练模型所需要的优化器以及损失的估算方法。在keras中，可以通过optimizer参数来指定优化器。经验证明，adam优化器具有非常良好的表现。loss='mse'表示使用均方误差（mse）作为损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译模型\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对模型进行训练，一下代码利用现有数据x和y对模型进行训练1000次，epochs表示训练轮次，batch_size表示每次有多少行数据参与训练，最后把整个训练过程记录到history中。程序运行后，在控制台会打印出每轮次的训练情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 41.7607 - val_loss: 27.4936\n",
      "Epoch 2/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 23.0322 - val_loss: 14.1658\n",
      "Epoch 3/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 12.9367 - val_loss: 9.4115\n",
      "Epoch 4/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 10.2151 - val_loss: 9.8998\n",
      "Epoch 5/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 11.3267 - val_loss: 11.9077\n",
      "Epoch 6/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 12.6352 - val_loss: 12.6378\n",
      "Epoch 7/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 12.4245 - val_loss: 11.3746\n",
      "Epoch 8/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 10.5695 - val_loss: 9.2587\n",
      "Epoch 9/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 8.3349 - val_loss: 7.3720\n",
      "Epoch 10/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 6.6561 - val_loss: 6.3847\n",
      "Epoch 11/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 5.6954 - val_loss: 6.2713\n",
      "Epoch 12/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 5.767 - 0s 287us/step - loss: 5.7721 - val_loss: 6.5126\n",
      "Epoch 13/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 5.9526 - val_loss: 6.5392\n",
      "Epoch 14/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 5.9486 - val_loss: 6.1723\n",
      "Epoch 15/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 5.5969 - val_loss: 5.5367\n",
      "Epoch 16/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 5.327 - 0s 189us/step - loss: 4.9964 - val_loss: 5.0232\n",
      "Epoch 17/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 4.5419 - val_loss: 4.8024\n",
      "Epoch 18/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 4.3314 - val_loss: 4.7887\n",
      "Epoch 19/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 4.339 - 0s 223us/step - loss: 4.3539 - val_loss: 4.8615\n",
      "Epoch 20/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 4.3921 - val_loss: 4.8370\n",
      "Epoch 21/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 4.3328 - val_loss: 4.6538\n",
      "Epoch 22/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 4.1595 - val_loss: 4.4076\n",
      "Epoch 23/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 3.9889 - val_loss: 4.1987\n",
      "Epoch 24/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 3.8668 - val_loss: 4.0925\n",
      "Epoch 25/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 3.8175 - val_loss: 4.0521\n",
      "Epoch 26/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 3.8169 - val_loss: 4.0218\n",
      "Epoch 27/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 3.7925 - val_loss: 3.9759\n",
      "Epoch 28/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.496 - 0s 193us/step - loss: 3.7339 - val_loss: 3.9312\n",
      "Epoch 29/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 3.6780 - val_loss: 3.9072\n",
      "Epoch 30/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 3.6271 - val_loss: 3.9116\n",
      "Epoch 31/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 3.5944 - val_loss: 3.9174\n",
      "Epoch 32/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 3.5795 - val_loss: 3.9101\n",
      "Epoch 33/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 3.5577 - val_loss: 3.8770\n",
      "Epoch 34/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 3.5280 - val_loss: 3.8376\n",
      "Epoch 35/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 3.4997 - val_loss: 3.8005\n",
      "Epoch 36/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 3.4804 - val_loss: 3.7731\n",
      "Epoch 37/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 3.4593 - val_loss: 3.7574\n",
      "Epoch 38/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 3.4402 - val_loss: 3.7440\n",
      "Epoch 39/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 3.4189 - val_loss: 3.7189\n",
      "Epoch 40/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 3.3981 - val_loss: 3.6990\n",
      "Epoch 41/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 3.3747 - val_loss: 3.6776\n",
      "Epoch 42/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 3.3620 - val_loss: 3.6595\n",
      "Epoch 43/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 3.3401 - val_loss: 3.6384\n",
      "Epoch 44/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 3.3214 - val_loss: 3.6090\n",
      "Epoch 45/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 3.3047 - val_loss: 3.5752\n",
      "Epoch 46/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 3.2847 - val_loss: 3.5485\n",
      "Epoch 47/1000\n",
      "160/160 [==============================] - 0s 176us/step - loss: 3.2728 - val_loss: 3.5219\n",
      "Epoch 48/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.850 - 0s 212us/step - loss: 3.2586 - val_loss: 3.5021\n",
      "Epoch 49/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 3.2442 - val_loss: 3.4861\n",
      "Epoch 50/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 3.2271 - val_loss: 3.4675\n",
      "Epoch 51/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 3.2114 - val_loss: 3.4521\n",
      "Epoch 52/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 3.1972 - val_loss: 3.4364\n",
      "Epoch 53/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 3.1830 - val_loss: 3.4192\n",
      "Epoch 54/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 3.1675 - val_loss: 3.4007\n",
      "Epoch 55/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 3.1558 - val_loss: 3.3721\n",
      "Epoch 56/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 3.1450 - val_loss: 3.3449\n",
      "Epoch 57/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.856 - 0s 191us/step - loss: 3.1275 - val_loss: 3.3256\n",
      "Epoch 58/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 3.1153 - val_loss: 3.3099\n",
      "Epoch 59/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 3.1045 - val_loss: 3.2934\n",
      "Epoch 60/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 3.0940 - val_loss: 3.2833\n",
      "Epoch 61/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 3.0822 - val_loss: 3.2704\n",
      "Epoch 62/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 3.0723 - val_loss: 3.2564\n",
      "Epoch 63/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 3.0631 - val_loss: 3.2404\n",
      "Epoch 64/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 3.0511 - val_loss: 3.2228\n",
      "Epoch 65/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 3.0466 - val_loss: 3.2016\n",
      "Epoch 66/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 3.0389 - val_loss: 3.1866\n",
      "Epoch 67/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 3.0248 - val_loss: 3.1781\n",
      "Epoch 68/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 3.0161 - val_loss: 3.1818\n",
      "Epoch 69/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 3.0017 - val_loss: 3.1773\n",
      "Epoch 70/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.9949 - val_loss: 3.1725\n",
      "Epoch 71/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.9853 - val_loss: 3.1541\n",
      "Epoch 72/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.9750 - val_loss: 3.1402\n",
      "Epoch 73/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.9674 - val_loss: 3.1208\n",
      "Epoch 74/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 2.9584 - val_loss: 3.1147\n",
      "Epoch 75/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.9486 - val_loss: 3.1049\n",
      "Epoch 76/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.9450 - val_loss: 3.0931\n",
      "Epoch 77/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.9339 - val_loss: 3.0883\n",
      "Epoch 78/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 2.9250 - val_loss: 3.0908\n",
      "Epoch 79/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.532 - 0s 221us/step - loss: 2.9158 - val_loss: 3.0940\n",
      "Epoch 80/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.9099 - val_loss: 3.0944\n",
      "Epoch 81/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.9013 - val_loss: 3.0804\n",
      "Epoch 82/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.8910 - val_loss: 3.0713\n",
      "Epoch 83/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.8857 - val_loss: 3.0553\n",
      "Epoch 84/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.8756 - val_loss: 3.0502\n",
      "Epoch 85/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.8700 - val_loss: 3.0415\n",
      "Epoch 86/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 2.8631 - val_loss: 3.0476\n",
      "Epoch 87/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.8524 - val_loss: 3.0458\n",
      "Epoch 88/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.8445 - val_loss: 3.0413\n",
      "Epoch 89/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.706 - 0s 209us/step - loss: 2.8375 - val_loss: 3.0365\n",
      "Epoch 90/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 2.8321 - val_loss: 3.0360\n",
      "Epoch 91/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 2.8242 - val_loss: 3.0283\n",
      "Epoch 92/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 2.8187 - val_loss: 3.0150\n",
      "Epoch 93/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.8106 - val_loss: 3.0086\n",
      "Epoch 94/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.8045 - val_loss: 3.0069\n",
      "Epoch 95/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.7981 - val_loss: 3.0013\n",
      "Epoch 96/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 2.7929 - val_loss: 3.0014\n",
      "Epoch 97/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.7882 - val_loss: 3.0009\n",
      "Epoch 98/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.7808 - val_loss: 2.9876\n",
      "Epoch 99/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.7742 - val_loss: 2.9794\n",
      "Epoch 100/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 2.7679 - val_loss: 2.9756\n",
      "Epoch 101/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.7625 - val_loss: 2.9777\n",
      "Epoch 102/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.7560 - val_loss: 2.9762\n",
      "Epoch 103/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.7507 - val_loss: 2.9742\n",
      "Epoch 104/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.7461 - val_loss: 2.9714\n",
      "Epoch 105/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.7395 - val_loss: 2.9608\n",
      "Epoch 106/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.7397 - val_loss: 2.9432\n",
      "Epoch 107/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 2.7322 - val_loss: 2.9372\n",
      "Epoch 108/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.7258 - val_loss: 2.9380\n",
      "Epoch 109/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.7299 - val_loss: 2.9604\n",
      "Epoch 110/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.7167 - val_loss: 2.9633\n",
      "Epoch 111/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 2.7104 - val_loss: 2.9473\n",
      "Epoch 112/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.7038 - val_loss: 2.9354\n",
      "Epoch 113/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 2.6985 - val_loss: 2.9163\n",
      "Epoch 114/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.6970 - val_loss: 2.9045\n",
      "Epoch 115/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 2.6931 - val_loss: 2.9136\n",
      "Epoch 116/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.6827 - val_loss: 2.9188\n",
      "Epoch 117/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 2.6774 - val_loss: 2.9177\n",
      "Epoch 118/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.6719 - val_loss: 2.9159\n",
      "Epoch 119/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 2.6696 - val_loss: 2.9161\n",
      "Epoch 120/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.6659 - val_loss: 2.8966\n",
      "Epoch 121/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 2.6579 - val_loss: 2.8868\n",
      "Epoch 122/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 2.6525 - val_loss: 2.8861\n",
      "Epoch 123/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.6472 - val_loss: 2.8868\n",
      "Epoch 124/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 2.6446 - val_loss: 2.8948\n",
      "Epoch 125/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.6380 - val_loss: 2.8861\n",
      "Epoch 126/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.6351 - val_loss: 2.8749\n",
      "Epoch 127/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.6274 - val_loss: 2.8731\n",
      "Epoch 128/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 2.6249 - val_loss: 2.8700\n",
      "Epoch 129/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 2.6190 - val_loss: 2.8661\n",
      "Epoch 130/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.6150 - val_loss: 2.8712\n",
      "Epoch 131/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.6087 - val_loss: 2.8645\n",
      "Epoch 132/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.6088 - val_loss: 2.8659\n",
      "Epoch 133/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 2.5991 - val_loss: 2.8541\n",
      "Epoch 134/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 2.5947 - val_loss: 2.8411\n",
      "Epoch 135/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 2.5911 - val_loss: 2.8381\n",
      "Epoch 136/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.5857 - val_loss: 2.8335\n",
      "Epoch 137/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 2.5816 - val_loss: 2.8387\n",
      "Epoch 138/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.5755 - val_loss: 2.8409\n",
      "Epoch 139/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 2.5706 - val_loss: 2.8374\n",
      "Epoch 140/1000\n",
      "160/160 [==============================] - 0s 281us/step - loss: 2.5679 - val_loss: 2.8275\n",
      "Epoch 141/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.5615 - val_loss: 2.8222\n",
      "Epoch 142/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.5590 - val_loss: 2.8155\n",
      "Epoch 143/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.5523 - val_loss: 2.8183\n",
      "Epoch 144/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.5496 - val_loss: 2.8214\n",
      "Epoch 145/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 2.5440 - val_loss: 2.8078\n",
      "Epoch 146/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.305 - 0s 240us/step - loss: 2.5402 - val_loss: 2.8058\n",
      "Epoch 147/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.474 - 0s 202us/step - loss: 2.5337 - val_loss: 2.7974\n",
      "Epoch 148/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.5291 - val_loss: 2.7865\n",
      "Epoch 149/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.5250 - val_loss: 2.7784\n",
      "Epoch 150/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 2.5205 - val_loss: 2.7749\n",
      "Epoch 151/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.5154 - val_loss: 2.7771\n",
      "Epoch 152/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.5107 - val_loss: 2.7817\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 201us/step - loss: 2.5066 - val_loss: 2.7752\n",
      "Epoch 154/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 2.5024 - val_loss: 2.7727\n",
      "Epoch 155/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.4968 - val_loss: 2.7639\n",
      "Epoch 156/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.4921 - val_loss: 2.7497\n",
      "Epoch 157/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.4878 - val_loss: 2.7449\n",
      "Epoch 158/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 2.4855 - val_loss: 2.7409\n",
      "Epoch 159/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 2.4789 - val_loss: 2.7470\n",
      "Epoch 160/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.4746 - val_loss: 2.7541\n",
      "Epoch 161/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.4718 - val_loss: 2.7461\n",
      "Epoch 162/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.4718 - val_loss: 2.7351\n",
      "Epoch 163/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 2.4605 - val_loss: 2.7389\n",
      "Epoch 164/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.414 - 0s 227us/step - loss: 2.4563 - val_loss: 2.7397\n",
      "Epoch 165/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.4531 - val_loss: 2.7375\n",
      "Epoch 166/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.4473 - val_loss: 2.7255\n",
      "Epoch 167/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 2.4433 - val_loss: 2.7128\n",
      "Epoch 168/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 2.4388 - val_loss: 2.7118\n",
      "Epoch 169/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.4343 - val_loss: 2.7082\n",
      "Epoch 170/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.4296 - val_loss: 2.7086\n",
      "Epoch 171/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.4288 - val_loss: 2.7169\n",
      "Epoch 172/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.4232 - val_loss: 2.7008\n",
      "Epoch 173/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 2.4191 - val_loss: 2.6881\n",
      "Epoch 174/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 2.4134 - val_loss: 2.6915\n",
      "Epoch 175/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.4072 - val_loss: 2.6876\n",
      "Epoch 176/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.4037 - val_loss: 2.6824\n",
      "Epoch 177/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 2.3991 - val_loss: 2.6767\n",
      "Epoch 178/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.3938 - val_loss: 2.6771\n",
      "Epoch 179/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.3901 - val_loss: 2.6770\n",
      "Epoch 180/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 2.3864 - val_loss: 2.6686\n",
      "Epoch 181/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.3810 - val_loss: 2.6617\n",
      "Epoch 182/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.3782 - val_loss: 2.6563\n",
      "Epoch 183/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.3740 - val_loss: 2.6624\n",
      "Epoch 184/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.3684 - val_loss: 2.6559\n",
      "Epoch 185/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 2.3648 - val_loss: 2.6471\n",
      "Epoch 186/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.3663 - val_loss: 2.6558\n",
      "Epoch 187/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 2.3554 - val_loss: 2.6463\n",
      "Epoch 188/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.3508 - val_loss: 2.6306\n",
      "Epoch 189/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.3477 - val_loss: 2.6214\n",
      "Epoch 190/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.797 - 0s 218us/step - loss: 2.3436 - val_loss: 2.6275\n",
      "Epoch 191/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.3394 - val_loss: 2.6349\n",
      "Epoch 192/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.3334 - val_loss: 2.6270\n",
      "Epoch 193/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.3294 - val_loss: 2.6140\n",
      "Epoch 194/1000\n",
      "160/160 [==============================] - 0s 165us/step - loss: 2.3317 - val_loss: 2.6194\n",
      "Epoch 195/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 2.3189 - val_loss: 2.6101\n",
      "Epoch 196/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.3180 - val_loss: 2.6077\n",
      "Epoch 197/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 2.3106 - val_loss: 2.5954\n",
      "Epoch 198/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.3061 - val_loss: 2.5919\n",
      "Epoch 199/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.3019 - val_loss: 2.5939\n",
      "Epoch 200/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 2.2970 - val_loss: 2.5998\n",
      "Epoch 201/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 2.2927 - val_loss: 2.6051\n",
      "Epoch 202/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 2.2903 - val_loss: 2.6038\n",
      "Epoch 203/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 2.2848 - val_loss: 2.5857\n",
      "Epoch 204/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 2.2829 - val_loss: 2.5813\n",
      "Epoch 205/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 2.2749 - val_loss: 2.5707\n",
      "Epoch 206/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.2720 - val_loss: 2.5594\n",
      "Epoch 207/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 2.2742 - val_loss: 2.5551\n",
      "Epoch 208/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.2645 - val_loss: 2.5735\n",
      "Epoch 209/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.2607 - val_loss: 2.5852\n",
      "Epoch 210/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.2570 - val_loss: 2.5759\n",
      "Epoch 211/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 2.2512 - val_loss: 2.5509\n",
      "Epoch 212/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 2.2459 - val_loss: 2.5384\n",
      "Epoch 213/1000\n",
      "160/160 [==============================] - 0s 178us/step - loss: 2.2441 - val_loss: 2.5426\n",
      "Epoch 214/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.2366 - val_loss: 2.5448\n",
      "Epoch 215/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.2338 - val_loss: 2.5488\n",
      "Epoch 216/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 2.2278 - val_loss: 2.5360\n",
      "Epoch 217/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 2.2244 - val_loss: 2.5240\n",
      "Epoch 218/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 2.2193 - val_loss: 2.5243\n",
      "Epoch 219/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 2.2162 - val_loss: 2.5304\n",
      "Epoch 220/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 2.2111 - val_loss: 2.5242\n",
      "Epoch 221/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 2.2088 - val_loss: 2.5136\n",
      "Epoch 222/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.2094 - val_loss: 2.5244\n",
      "Epoch 223/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.2006 - val_loss: 2.5092\n",
      "Epoch 224/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 2.1934 - val_loss: 2.5038\n",
      "Epoch 225/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 2.1907 - val_loss: 2.5010\n",
      "Epoch 226/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.1849 - val_loss: 2.5058\n",
      "Epoch 227/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 2.1806 - val_loss: 2.5046\n",
      "Epoch 228/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 2.1769 - val_loss: 2.4974\n",
      "Epoch 229/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.1741 - val_loss: 2.4839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.1697 - val_loss: 2.4861\n",
      "Epoch 231/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.1659 - val_loss: 2.4751\n",
      "Epoch 232/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 2.1638 - val_loss: 2.4834\n",
      "Epoch 233/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 2.1559 - val_loss: 2.4801\n",
      "Epoch 234/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.1601 - val_loss: 2.4616\n",
      "Epoch 235/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 2.1478 - val_loss: 2.4658\n",
      "Epoch 236/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.1431 - val_loss: 2.4646\n",
      "Epoch 237/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.1386 - val_loss: 2.4692\n",
      "Epoch 238/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.1399 - val_loss: 2.4570\n",
      "Epoch 239/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 2.1310 - val_loss: 2.4612\n",
      "Epoch 240/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 2.1291 - val_loss: 2.4606\n",
      "Epoch 241/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 2.1235 - val_loss: 2.4516\n",
      "Epoch 242/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 2.1181 - val_loss: 2.4399\n",
      "Epoch 243/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 2.1143 - val_loss: 2.4300\n",
      "Epoch 244/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 2.1095 - val_loss: 2.4264\n",
      "Epoch 245/1000\n",
      "160/160 [==============================] - 0s 182us/step - loss: 2.1060 - val_loss: 2.4255\n",
      "Epoch 246/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 2.1007 - val_loss: 2.4293\n",
      "Epoch 247/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 2.0985 - val_loss: 2.4296\n",
      "Epoch 248/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 2.0922 - val_loss: 2.4326\n",
      "Epoch 249/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 2.0899 - val_loss: 2.4234\n",
      "Epoch 250/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 2.0830 - val_loss: 2.4241\n",
      "Epoch 251/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 2.0798 - val_loss: 2.4109\n",
      "Epoch 252/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 2.0731 - val_loss: 2.4044\n",
      "Epoch 253/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 2.0666 - val_loss: 2.4044\n",
      "Epoch 254/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 2.0615 - val_loss: 2.4062\n",
      "Epoch 255/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 2.0605 - val_loss: 2.4027\n",
      "Epoch 256/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 2.0586 - val_loss: 2.3837\n",
      "Epoch 257/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 2.0485 - val_loss: 2.3872\n",
      "Epoch 258/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 2.0466 - val_loss: 2.3979\n",
      "Epoch 259/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 2.0420 - val_loss: 2.3806\n",
      "Epoch 260/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 2.0348 - val_loss: 2.3776\n",
      "Epoch 261/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.0294 - val_loss: 2.3612\n",
      "Epoch 262/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 2.0277 - val_loss: 2.3653\n",
      "Epoch 263/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 2.0196 - val_loss: 2.3618\n",
      "Epoch 264/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 2.0249 - val_loss: 2.3509\n",
      "Epoch 265/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 2.0126 - val_loss: 2.3565\n",
      "Epoch 266/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 2.0236 - val_loss: 2.3734\n",
      "Epoch 267/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 2.0071 - val_loss: 2.3575\n",
      "Epoch 268/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 2.0006 - val_loss: 2.3304\n",
      "Epoch 269/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.9976 - val_loss: 2.3257\n",
      "Epoch 270/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.864 - 0s 213us/step - loss: 1.9938 - val_loss: 2.3307\n",
      "Epoch 271/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.9887 - val_loss: 2.3478\n",
      "Epoch 272/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.9851 - val_loss: 2.3487\n",
      "Epoch 273/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.9802 - val_loss: 2.3341\n",
      "Epoch 274/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.9793 - val_loss: 2.3253\n",
      "Epoch 275/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.9704 - val_loss: 2.3078\n",
      "Epoch 276/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.9713 - val_loss: 2.3024\n",
      "Epoch 277/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.9646 - val_loss: 2.3116\n",
      "Epoch 278/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.9586 - val_loss: 2.3280\n",
      "Epoch 279/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.9572 - val_loss: 2.3303\n",
      "Epoch 280/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.9526 - val_loss: 2.3141\n",
      "Epoch 281/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.9478 - val_loss: 2.3015\n",
      "Epoch 282/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 1.9498 - val_loss: 2.2882\n",
      "Epoch 283/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.9415 - val_loss: 2.2960\n",
      "Epoch 284/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.9369 - val_loss: 2.3078\n",
      "Epoch 285/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.9331 - val_loss: 2.2999\n",
      "Epoch 286/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.9322 - val_loss: 2.2931\n",
      "Epoch 287/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 1.9240 - val_loss: 2.2778\n",
      "Epoch 288/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 1.9207 - val_loss: 2.2690\n",
      "Epoch 289/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 1.9201 - val_loss: 2.2679\n",
      "Epoch 290/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.9153 - val_loss: 2.2847\n",
      "Epoch 291/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.9093 - val_loss: 2.2840\n",
      "Epoch 292/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.9055 - val_loss: 2.2712\n",
      "Epoch 293/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.8997 - val_loss: 2.2519\n",
      "Epoch 294/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.8968 - val_loss: 2.2417\n",
      "Epoch 295/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.8940 - val_loss: 2.2425\n",
      "Epoch 296/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.8946 - val_loss: 2.2632\n",
      "Epoch 297/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 1.8881 - val_loss: 2.2601\n",
      "Epoch 298/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.8829 - val_loss: 2.2404\n",
      "Epoch 299/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.8776 - val_loss: 2.2172\n",
      "Epoch 300/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.8745 - val_loss: 2.2175\n",
      "Epoch 301/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.8713 - val_loss: 2.2323\n",
      "Epoch 302/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.8646 - val_loss: 2.2377\n",
      "Epoch 303/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 1.8616 - val_loss: 2.2285\n",
      "Epoch 304/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.8540 - val_loss: 2.2103\n",
      "Epoch 305/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.8508 - val_loss: 2.2052\n",
      "Epoch 306/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.8484 - val_loss: 2.2011\n",
      "Epoch 307/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 226us/step - loss: 1.8421 - val_loss: 2.2066\n",
      "Epoch 308/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.8412 - val_loss: 2.2200\n",
      "Epoch 309/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.8367 - val_loss: 2.2046\n",
      "Epoch 310/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.8298 - val_loss: 2.1961\n",
      "Epoch 311/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.8254 - val_loss: 2.1897\n",
      "Epoch 312/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.8214 - val_loss: 2.1862\n",
      "Epoch 313/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.8174 - val_loss: 2.1860\n",
      "Epoch 314/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.8143 - val_loss: 2.1784\n",
      "Epoch 315/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 1.8091 - val_loss: 2.1750\n",
      "Epoch 316/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.8062 - val_loss: 2.1721\n",
      "Epoch 317/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.8024 - val_loss: 2.1852\n",
      "Epoch 318/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 1.7978 - val_loss: 2.1791\n",
      "Epoch 319/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 1.7941 - val_loss: 2.1595\n",
      "Epoch 320/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.7892 - val_loss: 2.1533\n",
      "Epoch 321/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.7885 - val_loss: 2.1593\n",
      "Epoch 322/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.7809 - val_loss: 2.1570\n",
      "Epoch 323/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.807 - 0s 215us/step - loss: 1.7769 - val_loss: 2.1467\n",
      "Epoch 324/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.7725 - val_loss: 2.1444\n",
      "Epoch 325/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.7698 - val_loss: 2.1485\n",
      "Epoch 326/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.7659 - val_loss: 2.1397\n",
      "Epoch 327/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.7598 - val_loss: 2.1395\n",
      "Epoch 328/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.7607 - val_loss: 2.1339\n",
      "Epoch 329/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.7522 - val_loss: 2.1439\n",
      "Epoch 330/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.7490 - val_loss: 2.1434\n",
      "Epoch 331/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.7449 - val_loss: 2.1278\n",
      "Epoch 332/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.7416 - val_loss: 2.1178\n",
      "Epoch 333/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.7392 - val_loss: 2.1217\n",
      "Epoch 334/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 1.7314 - val_loss: 2.1174\n",
      "Epoch 335/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.7278 - val_loss: 2.1137\n",
      "Epoch 336/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.7251 - val_loss: 2.1104\n",
      "Epoch 337/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.7240 - val_loss: 2.0957\n",
      "Epoch 338/1000\n",
      "160/160 [==============================] - 0s 251us/step - loss: 1.7167 - val_loss: 2.0978\n",
      "Epoch 339/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.7125 - val_loss: 2.1076\n",
      "Epoch 340/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.7091 - val_loss: 2.1062\n",
      "Epoch 341/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.7103 - val_loss: 2.0869\n",
      "Epoch 342/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.7034 - val_loss: 2.0818\n",
      "Epoch 343/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 1.6960 - val_loss: 2.0885\n",
      "Epoch 344/1000\n",
      "160/160 [==============================] - 0s 263us/step - loss: 1.6969 - val_loss: 2.1068\n",
      "Epoch 345/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 1.6935 - val_loss: 2.0846\n",
      "Epoch 346/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.6835 - val_loss: 2.0737\n",
      "Epoch 347/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.6797 - val_loss: 2.0663\n",
      "Epoch 348/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 1.6779 - val_loss: 2.0670\n",
      "Epoch 349/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.6738 - val_loss: 2.0753\n",
      "Epoch 350/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 1.6742 - val_loss: 2.0789\n",
      "Epoch 351/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.748 - 0s 205us/step - loss: 1.6669 - val_loss: 2.0567\n",
      "Epoch 352/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.6658 - val_loss: 2.0545\n",
      "Epoch 353/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.6632 - val_loss: 2.0417\n",
      "Epoch 354/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.6564 - val_loss: 2.0493\n",
      "Epoch 355/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 1.6746 - val_loss: 2.0669\n",
      "Epoch 356/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.6497 - val_loss: 2.0370\n",
      "Epoch 357/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.6404 - val_loss: 2.0241\n",
      "Epoch 358/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.6407 - val_loss: 2.0226\n",
      "Epoch 359/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.6356 - val_loss: 2.0266\n",
      "Epoch 360/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.6308 - val_loss: 2.0292\n",
      "Epoch 361/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.6273 - val_loss: 2.0336\n",
      "Epoch 362/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 1.6296 - val_loss: 2.0327\n",
      "Epoch 363/1000\n",
      "160/160 [==============================] - 0s 252us/step - loss: 1.6236 - val_loss: 2.0089\n",
      "Epoch 364/1000\n",
      "160/160 [==============================] - 0s 288us/step - loss: 1.6173 - val_loss: 2.0047\n",
      "Epoch 365/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 1.6205 - val_loss: 2.0121\n",
      "Epoch 366/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.6083 - val_loss: 2.0042\n",
      "Epoch 367/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.6078 - val_loss: 1.9967\n",
      "Epoch 368/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.6009 - val_loss: 1.9984\n",
      "Epoch 369/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.5974 - val_loss: 2.0014\n",
      "Epoch 370/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.5989 - val_loss: 2.0027\n",
      "Epoch 371/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.5932 - val_loss: 1.9909\n",
      "Epoch 372/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.6050 - val_loss: 1.9715\n",
      "Epoch 373/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.5890 - val_loss: 1.9783\n",
      "Epoch 374/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.5788 - val_loss: 1.9920\n",
      "Epoch 375/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.5770 - val_loss: 1.9969\n",
      "Epoch 376/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.5759 - val_loss: 1.9862\n",
      "Epoch 377/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.5695 - val_loss: 1.9665\n",
      "Epoch 378/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.5647 - val_loss: 1.9582\n",
      "Epoch 379/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.5627 - val_loss: 1.9562\n",
      "Epoch 380/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.5595 - val_loss: 1.9568\n",
      "Epoch 381/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 1.5606 - val_loss: 1.9690\n",
      "Epoch 382/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.5530 - val_loss: 1.9557\n",
      "Epoch 383/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 1.5486 - val_loss: 1.9433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 1.5462 - val_loss: 1.9396\n",
      "Epoch 385/1000\n",
      "160/160 [==============================] - 0s 273us/step - loss: 1.5434 - val_loss: 1.9490\n",
      "Epoch 386/1000\n",
      "160/160 [==============================] - 0s 275us/step - loss: 1.5376 - val_loss: 1.9438\n",
      "Epoch 387/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.5337 - val_loss: 1.9364\n",
      "Epoch 388/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.5304 - val_loss: 1.9333\n",
      "Epoch 389/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.5270 - val_loss: 1.9242\n",
      "Epoch 390/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.5252 - val_loss: 1.9213\n",
      "Epoch 391/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.5354 - val_loss: 1.9401\n",
      "Epoch 392/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.5196 - val_loss: 1.9241\n",
      "Epoch 393/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.5118 - val_loss: 1.9063\n",
      "Epoch 394/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.5128 - val_loss: 1.8993\n",
      "Epoch 395/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 1.5087 - val_loss: 1.9074\n",
      "Epoch 396/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.5015 - val_loss: 1.9190\n",
      "Epoch 397/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.5101 - val_loss: 1.9252\n",
      "Epoch 398/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.5019 - val_loss: 1.8921\n",
      "Epoch 399/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 1.5010 - val_loss: 1.8828\n",
      "Epoch 400/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.4917 - val_loss: 1.8900\n",
      "Epoch 401/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 1.4843 - val_loss: 1.9060\n",
      "Epoch 402/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 1.4870 - val_loss: 1.9046\n",
      "Epoch 403/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.4802 - val_loss: 1.8813\n",
      "Epoch 404/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.4792 - val_loss: 1.8693\n",
      "Epoch 405/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.4758 - val_loss: 1.8698\n",
      "Epoch 406/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.4708 - val_loss: 1.8749\n",
      "Epoch 407/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 1.4635 - val_loss: 1.8880\n",
      "Epoch 408/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.4658 - val_loss: 1.8793\n",
      "Epoch 409/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 1.4629 - val_loss: 1.8604\n",
      "Epoch 410/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.4567 - val_loss: 1.8546\n",
      "Epoch 411/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.4519 - val_loss: 1.8607\n",
      "Epoch 412/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 1.4482 - val_loss: 1.8607\n",
      "Epoch 413/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.4442 - val_loss: 1.8580\n",
      "Epoch 414/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.4413 - val_loss: 1.8500\n",
      "Epoch 415/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.4385 - val_loss: 1.8451\n",
      "Epoch 416/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.4350 - val_loss: 1.8406\n",
      "Epoch 417/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.4335 - val_loss: 1.8370\n",
      "Epoch 418/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.4285 - val_loss: 1.8429\n",
      "Epoch 419/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.4270 - val_loss: 1.8421\n",
      "Epoch 420/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.4230 - val_loss: 1.8301\n",
      "Epoch 421/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.4180 - val_loss: 1.8203\n",
      "Epoch 422/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.4166 - val_loss: 1.8222\n",
      "Epoch 423/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.4170 - val_loss: 1.8270\n",
      "Epoch 424/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.4123 - val_loss: 1.8114\n",
      "Epoch 425/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.4076 - val_loss: 1.8046\n",
      "Epoch 426/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.4019 - val_loss: 1.8095\n",
      "Epoch 427/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.3992 - val_loss: 1.8084\n",
      "Epoch 428/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.3982 - val_loss: 1.7989\n",
      "Epoch 429/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.3983 - val_loss: 1.8052\n",
      "Epoch 430/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.3892 - val_loss: 1.7959\n",
      "Epoch 431/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.3865 - val_loss: 1.7897\n",
      "Epoch 432/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 1.3954 - val_loss: 1.7808\n",
      "Epoch 433/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.3806 - val_loss: 1.7900\n",
      "Epoch 434/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.3771 - val_loss: 1.8031\n",
      "Epoch 435/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 1.3772 - val_loss: 1.7860\n",
      "Epoch 436/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.3748 - val_loss: 1.7686\n",
      "Epoch 437/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.3677 - val_loss: 1.7681\n",
      "Epoch 438/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 1.3634 - val_loss: 1.7721\n",
      "Epoch 439/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.3762 - val_loss: 1.7834\n",
      "Epoch 440/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 1.3589 - val_loss: 1.7589\n",
      "Epoch 441/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 1.3586 - val_loss: 1.7463\n",
      "Epoch 442/1000\n",
      "160/160 [==============================] - 0s 263us/step - loss: 1.3679 - val_loss: 1.7441\n",
      "Epoch 443/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.3554 - val_loss: 1.7745\n",
      "Epoch 444/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.3545 - val_loss: 1.7716\n",
      "Epoch 445/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.3445 - val_loss: 1.7420\n",
      "Epoch 446/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.3409 - val_loss: 1.7314\n",
      "Epoch 447/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 1.3430 - val_loss: 1.7293\n",
      "Epoch 448/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.3437 - val_loss: 1.7545\n",
      "Epoch 449/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.3331 - val_loss: 1.7423\n",
      "Epoch 450/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.3271 - val_loss: 1.7276\n",
      "Epoch 451/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.3213 - val_loss: 1.7170\n",
      "Epoch 452/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.3220 - val_loss: 1.7128\n",
      "Epoch 453/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 1.3168 - val_loss: 1.7161\n",
      "Epoch 454/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.3140 - val_loss: 1.7292\n",
      "Epoch 455/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.3129 - val_loss: 1.7166\n",
      "Epoch 456/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.3059 - val_loss: 1.7015\n",
      "Epoch 457/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.3037 - val_loss: 1.6990\n",
      "Epoch 458/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.3013 - val_loss: 1.7007\n",
      "Epoch 459/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 1.3012 - val_loss: 1.6954\n",
      "Epoch 460/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 1.2927 - val_loss: 1.6993\n",
      "Epoch 461/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.2933 - val_loss: 1.7001\n",
      "Epoch 462/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.2979 - val_loss: 1.6837\n",
      "Epoch 463/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.2850 - val_loss: 1.6808\n",
      "Epoch 464/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.2807 - val_loss: 1.6851\n",
      "Epoch 465/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.2782 - val_loss: 1.6843\n",
      "Epoch 466/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.2764 - val_loss: 1.6755\n",
      "Epoch 467/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 1.2717 - val_loss: 1.6716\n",
      "Epoch 468/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 1.2693 - val_loss: 1.6719\n",
      "Epoch 469/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.2662 - val_loss: 1.6686\n",
      "Epoch 470/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.2659 - val_loss: 1.6674\n",
      "Epoch 471/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 1.2602 - val_loss: 1.6565\n",
      "Epoch 472/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.2572 - val_loss: 1.6524\n",
      "Epoch 473/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.2539 - val_loss: 1.6531\n",
      "Epoch 474/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 1.2508 - val_loss: 1.6536\n",
      "Epoch 475/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 1.2516 - val_loss: 1.6451\n",
      "Epoch 476/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 1.2448 - val_loss: 1.6464\n",
      "Epoch 477/1000\n",
      "160/160 [==============================] - 0s 268us/step - loss: 1.2436 - val_loss: 1.6445\n",
      "Epoch 478/1000\n",
      "160/160 [==============================] - 0s 261us/step - loss: 1.2392 - val_loss: 1.6349\n",
      "Epoch 479/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 1.2362 - val_loss: 1.6314\n",
      "Epoch 480/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 1.2339 - val_loss: 1.6342\n",
      "Epoch 481/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 1.2307 - val_loss: 1.6340\n",
      "Epoch 482/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 1.2279 - val_loss: 1.6274\n",
      "Epoch 483/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 1.2261 - val_loss: 1.6221\n",
      "Epoch 484/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.2257 - val_loss: 1.6152\n",
      "Epoch 485/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 1.2249 - val_loss: 1.6201\n",
      "Epoch 486/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.2171 - val_loss: 1.6155\n",
      "Epoch 487/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 1.2131 - val_loss: 1.6104\n",
      "Epoch 488/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.2137 - val_loss: 1.6044\n",
      "Epoch 489/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.2086 - val_loss: 1.6029\n",
      "Epoch 490/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 1.2039 - val_loss: 1.6035\n",
      "Epoch 491/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.2072 - val_loss: 1.6072\n",
      "Epoch 492/1000\n",
      "160/160 [==============================] - 0s 272us/step - loss: 1.2073 - val_loss: 1.5881\n",
      "Epoch 493/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 1.2029 - val_loss: 1.5850\n",
      "Epoch 494/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.1952 - val_loss: 1.5977\n",
      "Epoch 495/1000\n",
      "160/160 [==============================] - 0s 261us/step - loss: 1.1919 - val_loss: 1.5927\n",
      "Epoch 496/1000\n",
      "160/160 [==============================] - 0s 271us/step - loss: 1.1936 - val_loss: 1.5758\n",
      "Epoch 497/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.1856 - val_loss: 1.5737\n",
      "Epoch 498/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.1815 - val_loss: 1.5746\n",
      "Epoch 499/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 1.1785 - val_loss: 1.5775\n",
      "Epoch 500/1000\n",
      "160/160 [==============================] - 0s 273us/step - loss: 1.1768 - val_loss: 1.5766\n",
      "Epoch 501/1000\n",
      "160/160 [==============================] - 0s 270us/step - loss: 1.1741 - val_loss: 1.5677\n",
      "Epoch 502/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.1704 - val_loss: 1.5658\n",
      "Epoch 503/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.1680 - val_loss: 1.5624\n",
      "Epoch 504/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 1.1662 - val_loss: 1.5594\n",
      "Epoch 505/1000\n",
      "160/160 [==============================] - 0s 261us/step - loss: 1.1618 - val_loss: 1.5583\n",
      "Epoch 506/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 1.1593 - val_loss: 1.5553\n",
      "Epoch 507/1000\n",
      "160/160 [==============================] - 0s 241us/step - loss: 1.1565 - val_loss: 1.5494\n",
      "Epoch 508/1000\n",
      "160/160 [==============================] - 0s 261us/step - loss: 1.1529 - val_loss: 1.5436\n",
      "Epoch 509/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.1507 - val_loss: 1.5412\n",
      "Epoch 510/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 1.1489 - val_loss: 1.5381\n",
      "Epoch 511/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.1466 - val_loss: 1.5358\n",
      "Epoch 512/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.1422 - val_loss: 1.5365\n",
      "Epoch 513/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 1.1464 - val_loss: 1.5422\n",
      "Epoch 514/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.1399 - val_loss: 1.5266\n",
      "Epoch 515/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 1.1365 - val_loss: 1.5217\n",
      "Epoch 516/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.1327 - val_loss: 1.5226\n",
      "Epoch 517/1000\n",
      "160/160 [==============================] - 0s 230us/step - loss: 1.1321 - val_loss: 1.5218\n",
      "Epoch 518/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.1415 - val_loss: 1.5352\n",
      "Epoch 519/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.1285 - val_loss: 1.5138\n",
      "Epoch 520/1000\n",
      "160/160 [==============================] - 0s 252us/step - loss: 1.1214 - val_loss: 1.5079\n",
      "Epoch 521/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.1224 - val_loss: 1.5059\n",
      "Epoch 522/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.1236 - val_loss: 1.5156\n",
      "Epoch 523/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 1.1158 - val_loss: 1.5034\n",
      "Epoch 524/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 1.1137 - val_loss: 1.4937\n",
      "Epoch 525/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.1098 - val_loss: 1.4931\n",
      "Epoch 526/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 1.1064 - val_loss: 1.4999\n",
      "Epoch 527/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 1.1041 - val_loss: 1.4932\n",
      "Epoch 528/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 1.0990 - val_loss: 1.4873\n",
      "Epoch 529/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 1.1069 - val_loss: 1.4823\n",
      "Epoch 530/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.1019 - val_loss: 1.4939\n",
      "Epoch 531/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.0934 - val_loss: 1.4814\n",
      "Epoch 532/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 1.0885 - val_loss: 1.4766\n",
      "Epoch 533/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 1.0869 - val_loss: 1.4719\n",
      "Epoch 534/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 1.0832 - val_loss: 1.4707\n",
      "Epoch 535/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 1.0789 - val_loss: 1.4722\n",
      "Epoch 536/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.0825 - val_loss: 1.4717\n",
      "Epoch 537/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.0753 - val_loss: 1.4568\n",
      "Epoch 538/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 223us/step - loss: 1.0768 - val_loss: 1.4544\n",
      "Epoch 539/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 1.0708 - val_loss: 1.4561\n",
      "Epoch 540/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 1.0672 - val_loss: 1.4627\n",
      "Epoch 541/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 1.0661 - val_loss: 1.4509\n",
      "Epoch 542/1000\n",
      "160/160 [==============================] - 0s 260us/step - loss: 1.0614 - val_loss: 1.4448\n",
      "Epoch 543/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.0600 - val_loss: 1.4413\n",
      "Epoch 544/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 1.0557 - val_loss: 1.4410\n",
      "Epoch 545/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 1.0542 - val_loss: 1.4416\n",
      "Epoch 546/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 1.0506 - val_loss: 1.4354\n",
      "Epoch 547/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 1.0623 - val_loss: 1.4319\n",
      "Epoch 548/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 1.0467 - val_loss: 1.4336\n",
      "Epoch 549/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 1.0546 - val_loss: 1.4504\n",
      "Epoch 550/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.0519 - val_loss: 1.4295\n",
      "Epoch 551/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.0342 - val_loss: 1.4220\n",
      "Epoch 552/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.0458 - val_loss: 1.4175\n",
      "Epoch 553/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.0350 - val_loss: 1.4186\n",
      "Epoch 554/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 1.0402 - val_loss: 1.4320\n",
      "Epoch 555/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.0395 - val_loss: 1.4052\n",
      "Epoch 556/1000\n",
      "160/160 [==============================] - 0s 170us/step - loss: 1.0297 - val_loss: 1.4023\n",
      "Epoch 557/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 1.0307 - val_loss: 1.4026\n",
      "Epoch 558/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 1.0213 - val_loss: 1.4015\n",
      "Epoch 559/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 1.0188 - val_loss: 1.4018\n",
      "Epoch 560/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 1.0206 - val_loss: 1.3947\n",
      "Epoch 561/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 1.0135 - val_loss: 1.3943\n",
      "Epoch 562/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 1.0104 - val_loss: 1.3926\n",
      "Epoch 563/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 1.0077 - val_loss: 1.3904\n",
      "Epoch 564/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 1.0130 - val_loss: 1.3824\n",
      "Epoch 565/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 1.0050 - val_loss: 1.3859\n",
      "Epoch 566/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 1.0015 - val_loss: 1.3821\n",
      "Epoch 567/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 1.0005 - val_loss: 1.3754\n",
      "Epoch 568/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.9977 - val_loss: 1.3752\n",
      "Epoch 569/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.9984 - val_loss: 1.3801\n",
      "Epoch 570/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.9924 - val_loss: 1.3680\n",
      "Epoch 571/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.9908 - val_loss: 1.3633\n",
      "Epoch 572/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.9888 - val_loss: 1.3641\n",
      "Epoch 573/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.9835 - val_loss: 1.3625\n",
      "Epoch 574/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.9838 - val_loss: 1.3640\n",
      "Epoch 575/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.9793 - val_loss: 1.3561\n",
      "Epoch 576/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.9768 - val_loss: 1.3540\n",
      "Epoch 577/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.9741 - val_loss: 1.3553\n",
      "Epoch 578/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.9736 - val_loss: 1.3494\n",
      "Epoch 579/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.9682 - val_loss: 1.3482\n",
      "Epoch 580/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.9664 - val_loss: 1.3433\n",
      "Epoch 581/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.9660 - val_loss: 1.3396\n",
      "Epoch 582/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.9619 - val_loss: 1.3386\n",
      "Epoch 583/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.9647 - val_loss: 1.3430\n",
      "Epoch 584/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.9577 - val_loss: 1.3328\n",
      "Epoch 585/1000\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.9539 - val_loss: 1.3302\n",
      "Epoch 586/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.9554 - val_loss: 1.3268\n",
      "Epoch 587/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.9494 - val_loss: 1.3302\n",
      "Epoch 588/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.9483 - val_loss: 1.3268\n",
      "Epoch 589/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.9449 - val_loss: 1.3218\n",
      "Epoch 590/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.9421 - val_loss: 1.3175\n",
      "Epoch 591/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.9410 - val_loss: 1.3151\n",
      "Epoch 592/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.9419 - val_loss: 1.3123\n",
      "Epoch 593/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.9350 - val_loss: 1.3139\n",
      "Epoch 594/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.9355 - val_loss: 1.3050\n",
      "Epoch 595/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.9309 - val_loss: 1.3004\n",
      "Epoch 596/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.9335 - val_loss: 1.2990\n",
      "Epoch 597/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.9335 - val_loss: 1.2951\n",
      "Epoch 598/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.9286 - val_loss: 1.2982\n",
      "Epoch 599/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.9230 - val_loss: 1.2928\n",
      "Epoch 600/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.9212 - val_loss: 1.2907\n",
      "Epoch 601/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.9181 - val_loss: 1.2894\n",
      "Epoch 602/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.9159 - val_loss: 1.2872\n",
      "Epoch 603/1000\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.9123 - val_loss: 1.2813\n",
      "Epoch 604/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.9104 - val_loss: 1.2788\n",
      "Epoch 605/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.9151 - val_loss: 1.2749\n",
      "Epoch 606/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.9059 - val_loss: 1.2789\n",
      "Epoch 607/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.9068 - val_loss: 1.2766\n",
      "Epoch 608/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.9039 - val_loss: 1.2676\n",
      "Epoch 609/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.8990 - val_loss: 1.2656\n",
      "Epoch 610/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.9002 - val_loss: 1.2668\n",
      "Epoch 611/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.8959 - val_loss: 1.2656\n",
      "Epoch 612/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.8940 - val_loss: 1.2590\n",
      "Epoch 613/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.8905 - val_loss: 1.2557\n",
      "Epoch 614/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.8898 - val_loss: 1.2545\n",
      "Epoch 615/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.8878 - val_loss: 1.2497\n",
      "Epoch 616/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.8833 - val_loss: 1.2481\n",
      "Epoch 617/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.8816 - val_loss: 1.2478\n",
      "Epoch 618/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.8805 - val_loss: 1.2437\n",
      "Epoch 619/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.8769 - val_loss: 1.2421\n",
      "Epoch 620/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.8747 - val_loss: 1.2392\n",
      "Epoch 621/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.8725 - val_loss: 1.2361\n",
      "Epoch 622/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.8704 - val_loss: 1.2336\n",
      "Epoch 623/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.8680 - val_loss: 1.2306\n",
      "Epoch 624/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.8662 - val_loss: 1.2288\n",
      "Epoch 625/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.8652 - val_loss: 1.2251\n",
      "Epoch 626/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.8631 - val_loss: 1.2256\n",
      "Epoch 627/1000\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.8612 - val_loss: 1.2228\n",
      "Epoch 628/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.8580 - val_loss: 1.2209\n",
      "Epoch 629/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.8552 - val_loss: 1.2187\n",
      "Epoch 630/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.8560 - val_loss: 1.2158\n",
      "Epoch 631/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.8512 - val_loss: 1.2105\n",
      "Epoch 632/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.8516 - val_loss: 1.2065\n",
      "Epoch 633/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.8478 - val_loss: 1.2049\n",
      "Epoch 634/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.8442 - val_loss: 1.2056\n",
      "Epoch 635/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.8428 - val_loss: 1.2032\n",
      "Epoch 636/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.8416 - val_loss: 1.2009\n",
      "Epoch 637/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.8390 - val_loss: 1.1984\n",
      "Epoch 638/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.8361 - val_loss: 1.1937\n",
      "Epoch 639/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.8347 - val_loss: 1.1888\n",
      "Epoch 640/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.8319 - val_loss: 1.1864\n",
      "Epoch 641/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.8451 - val_loss: 1.1857\n",
      "Epoch 642/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.8269 - val_loss: 1.1949\n",
      "Epoch 643/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.8341 - val_loss: 1.1932\n",
      "Epoch 644/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.8247 - val_loss: 1.1789\n",
      "Epoch 645/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.8290 - val_loss: 1.1760\n",
      "Epoch 646/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.8215 - val_loss: 1.1714\n",
      "Epoch 647/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.8234 - val_loss: 1.1764\n",
      "Epoch 648/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.8216 - val_loss: 1.1654\n",
      "Epoch 649/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.8133 - val_loss: 1.1644\n",
      "Epoch 650/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.8169 - val_loss: 1.1646\n",
      "Epoch 651/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.8152 - val_loss: 1.1607\n",
      "Epoch 652/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.8130 - val_loss: 1.1605\n",
      "Epoch 653/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.8075 - val_loss: 1.1550\n",
      "Epoch 654/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.8044 - val_loss: 1.1547\n",
      "Epoch 655/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.8036 - val_loss: 1.1539\n",
      "Epoch 656/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.8022 - val_loss: 1.1550\n",
      "Epoch 657/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.7979 - val_loss: 1.1470\n",
      "Epoch 658/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.7949 - val_loss: 1.1424\n",
      "Epoch 659/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.7951 - val_loss: 1.1375\n",
      "Epoch 660/1000\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.7980 - val_loss: 1.1339\n",
      "Epoch 661/1000\n",
      "160/160 [==============================] - 0s 149us/step - loss: 0.7911 - val_loss: 1.1367\n",
      "Epoch 662/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.688 - 0s 189us/step - loss: 0.7887 - val_loss: 1.1338\n",
      "Epoch 663/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.7902 - val_loss: 1.1323\n",
      "Epoch 664/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.7911 - val_loss: 1.1293\n",
      "Epoch 665/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.7821 - val_loss: 1.1359\n",
      "Epoch 666/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.7862 - val_loss: 1.1288\n",
      "Epoch 667/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.7772 - val_loss: 1.1266\n",
      "Epoch 668/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.7831 - val_loss: 1.1229\n",
      "Epoch 669/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.7749 - val_loss: 1.1273\n",
      "Epoch 670/1000\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.7770 - val_loss: 1.1209\n",
      "Epoch 671/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.7700 - val_loss: 1.1146\n",
      "Epoch 672/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.7701 - val_loss: 1.1127\n",
      "Epoch 673/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.7693 - val_loss: 1.1092\n",
      "Epoch 674/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.7650 - val_loss: 1.1094\n",
      "Epoch 675/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.7707 - val_loss: 1.1038\n",
      "Epoch 676/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.7613 - val_loss: 1.1057\n",
      "Epoch 677/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.7722 - val_loss: 1.0993\n",
      "Epoch 678/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.7592 - val_loss: 1.0988\n",
      "Epoch 679/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.7557 - val_loss: 1.0935\n",
      "Epoch 680/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.7539 - val_loss: 1.0903\n",
      "Epoch 681/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.7521 - val_loss: 1.0864\n",
      "Epoch 682/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.7537 - val_loss: 1.0857\n",
      "Epoch 683/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.7521 - val_loss: 1.0916\n",
      "Epoch 684/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.7487 - val_loss: 1.0856\n",
      "Epoch 685/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.7477 - val_loss: 1.0831\n",
      "Epoch 686/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.7424 - val_loss: 1.0803\n",
      "Epoch 687/1000\n",
      "160/160 [==============================] - 0s 259us/step - loss: 0.7470 - val_loss: 1.0769\n",
      "Epoch 688/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.7408 - val_loss: 1.0751\n",
      "Epoch 689/1000\n",
      "160/160 [==============================] - 0s 260us/step - loss: 0.7480 - val_loss: 1.0756\n",
      "Epoch 690/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.7341 - val_loss: 1.0785\n",
      "Epoch 691/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.7482 - val_loss: 1.0777\n",
      "Epoch 692/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 226us/step - loss: 0.7335 - val_loss: 1.0684\n",
      "Epoch 693/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 0.7359 - val_loss: 1.0637\n",
      "Epoch 694/1000\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.7296 - val_loss: 1.0621\n",
      "Epoch 695/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.7277 - val_loss: 1.0624\n",
      "Epoch 696/1000\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.7300 - val_loss: 1.0573\n",
      "Epoch 697/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.7235 - val_loss: 1.0551\n",
      "Epoch 698/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.7310 - val_loss: 1.0515\n",
      "Epoch 699/1000\n",
      "160/160 [==============================] - 0s 253us/step - loss: 0.7200 - val_loss: 1.0465\n",
      "Epoch 700/1000\n",
      "160/160 [==============================] - 0s 266us/step - loss: 0.7238 - val_loss: 1.0441\n",
      "Epoch 701/1000\n",
      "160/160 [==============================] - 0s 263us/step - loss: 0.7161 - val_loss: 1.0435\n",
      "Epoch 702/1000\n",
      "160/160 [==============================] - 0s 268us/step - loss: 0.7151 - val_loss: 1.0447\n",
      "Epoch 703/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 0.7157 - val_loss: 1.0469\n",
      "Epoch 704/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.7148 - val_loss: 1.0447\n",
      "Epoch 705/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.7122 - val_loss: 1.0421\n",
      "Epoch 706/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.7092 - val_loss: 1.0377\n",
      "Epoch 707/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.7099 - val_loss: 1.0358\n",
      "Epoch 708/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.7085 - val_loss: 1.0365\n",
      "Epoch 709/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.7056 - val_loss: 1.0291\n",
      "Epoch 710/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.7010 - val_loss: 1.0252\n",
      "Epoch 711/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6992 - val_loss: 1.0231\n",
      "Epoch 712/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6984 - val_loss: 1.0227\n",
      "Epoch 713/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6958 - val_loss: 1.0211\n",
      "Epoch 714/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.6962 - val_loss: 1.0171\n",
      "Epoch 715/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6961 - val_loss: 1.0140\n",
      "Epoch 716/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.6921 - val_loss: 1.0156\n",
      "Epoch 717/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6909 - val_loss: 1.0133\n",
      "Epoch 718/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.6883 - val_loss: 1.0111\n",
      "Epoch 719/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6865 - val_loss: 1.0073\n",
      "Epoch 720/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6848 - val_loss: 1.0042\n",
      "Epoch 721/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.6859 - val_loss: 1.0034\n",
      "Epoch 722/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6819 - val_loss: 1.0009\n",
      "Epoch 723/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6790 - val_loss: 1.0035\n",
      "Epoch 724/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.6791 - val_loss: 1.0028\n",
      "Epoch 725/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6757 - val_loss: 1.0001\n",
      "Epoch 726/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6740 - val_loss: 0.9955\n",
      "Epoch 727/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6735 - val_loss: 0.9899\n",
      "Epoch 728/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6717 - val_loss: 0.9869\n",
      "Epoch 729/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6689 - val_loss: 0.9846\n",
      "Epoch 730/1000\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6679 - val_loss: 0.9843\n",
      "Epoch 731/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.6692 - val_loss: 0.9842\n",
      "Epoch 732/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6687 - val_loss: 0.9822\n",
      "Epoch 733/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6628 - val_loss: 0.9813\n",
      "Epoch 734/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6637 - val_loss: 0.9805\n",
      "Epoch 735/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6584 - val_loss: 0.9803\n",
      "Epoch 736/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6610 - val_loss: 0.9757\n",
      "Epoch 737/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6615 - val_loss: 0.9768\n",
      "Epoch 738/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.6582 - val_loss: 0.9730\n",
      "Epoch 739/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6537 - val_loss: 0.9744\n",
      "Epoch 740/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6548 - val_loss: 0.9690\n",
      "Epoch 741/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6563 - val_loss: 0.9656\n",
      "Epoch 742/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6530 - val_loss: 0.9650\n",
      "Epoch 743/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6525 - val_loss: 0.9622\n",
      "Epoch 744/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6517 - val_loss: 0.9578\n",
      "Epoch 745/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6459 - val_loss: 0.9520\n",
      "Epoch 746/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6434 - val_loss: 0.9534\n",
      "Epoch 747/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6435 - val_loss: 0.9524\n",
      "Epoch 748/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6420 - val_loss: 0.9535\n",
      "Epoch 749/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6412 - val_loss: 0.9503\n",
      "Epoch 750/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6379 - val_loss: 0.9490\n",
      "Epoch 751/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6377 - val_loss: 0.9465\n",
      "Epoch 752/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 0.6361 - val_loss: 0.9426\n",
      "Epoch 753/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6349 - val_loss: 0.9400\n",
      "Epoch 754/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6363 - val_loss: 0.9405\n",
      "Epoch 755/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6307 - val_loss: 0.9382\n",
      "Epoch 756/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6291 - val_loss: 0.9369\n",
      "Epoch 757/1000\n",
      "160/160 [==============================] - 0s 258us/step - loss: 0.6310 - val_loss: 0.9365\n",
      "Epoch 758/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6283 - val_loss: 0.9344\n",
      "Epoch 759/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6240 - val_loss: 0.9317\n",
      "Epoch 760/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6305 - val_loss: 0.9274\n",
      "Epoch 761/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6191 - val_loss: 0.9262\n",
      "Epoch 762/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6253 - val_loss: 0.9250\n",
      "Epoch 763/1000\n",
      "160/160 [==============================] - 0s 278us/step - loss: 0.6211 - val_loss: 0.9244\n",
      "Epoch 764/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6203 - val_loss: 0.9186\n",
      "Epoch 765/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6141 - val_loss: 0.9180\n",
      "Epoch 766/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6151 - val_loss: 0.9158\n",
      "Epoch 767/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6188 - val_loss: 0.9143\n",
      "Epoch 768/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6138 - val_loss: 0.9217\n",
      "Epoch 769/1000\n",
      "160/160 [==============================] - 0s 254us/step - loss: 0.6172 - val_loss: 0.9102\n",
      "Epoch 770/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.6071 - val_loss: 0.9107\n",
      "Epoch 771/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6096 - val_loss: 0.9081\n",
      "Epoch 772/1000\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6098 - val_loss: 0.9158\n",
      "Epoch 773/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.6060 - val_loss: 0.9033\n",
      "Epoch 774/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6253 - val_loss: 0.9100\n",
      "Epoch 775/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6023 - val_loss: 0.9045\n",
      "Epoch 776/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6050 - val_loss: 0.9016\n",
      "Epoch 777/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.5986 - val_loss: 0.8986\n",
      "Epoch 778/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6043 - val_loss: 0.9002\n",
      "Epoch 779/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.5937 - val_loss: 0.9020\n",
      "Epoch 780/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.5961 - val_loss: 0.8977\n",
      "Epoch 781/1000\n",
      "160/160 [==============================] - 0s 265us/step - loss: 0.5918 - val_loss: 0.8905\n",
      "Epoch 782/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.5894 - val_loss: 0.8935\n",
      "Epoch 783/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.5917 - val_loss: 0.8873\n",
      "Epoch 784/1000\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.5892 - val_loss: 0.8931\n",
      "Epoch 785/1000\n",
      "160/160 [==============================] - 0s 265us/step - loss: 0.5892 - val_loss: 0.8833\n",
      "Epoch 786/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.5831 - val_loss: 0.8830\n",
      "Epoch 787/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.5832 - val_loss: 0.8807\n",
      "Epoch 788/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.5830 - val_loss: 0.8822\n",
      "Epoch 789/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.5808 - val_loss: 0.8791\n",
      "Epoch 790/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.5793 - val_loss: 0.8757\n",
      "Epoch 791/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.5803 - val_loss: 0.8728\n",
      "Epoch 792/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5777 - val_loss: 0.8693\n",
      "Epoch 793/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.5791 - val_loss: 0.8674\n",
      "Epoch 794/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.5736 - val_loss: 0.8651\n",
      "Epoch 795/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 0.5740 - val_loss: 0.8662\n",
      "Epoch 796/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.5772 - val_loss: 0.8652\n",
      "Epoch 797/1000\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5720 - val_loss: 0.8671\n",
      "Epoch 798/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.5721 - val_loss: 0.8631\n",
      "Epoch 799/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.5843 - val_loss: 0.8587\n",
      "Epoch 800/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.5709 - val_loss: 0.8621\n",
      "Epoch 801/1000\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.5697 - val_loss: 0.8553\n",
      "Epoch 802/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.5693 - val_loss: 0.8519\n",
      "Epoch 803/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.5616 - val_loss: 0.8502\n",
      "Epoch 804/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.5665 - val_loss: 0.8499\n",
      "Epoch 805/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.5634 - val_loss: 0.8524\n",
      "Epoch 806/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.5633 - val_loss: 0.8449\n",
      "Epoch 807/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.5601 - val_loss: 0.8467\n",
      "Epoch 808/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.5593 - val_loss: 0.8440\n",
      "Epoch 809/1000\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.5577 - val_loss: 0.8513\n",
      "Epoch 810/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.5594 - val_loss: 0.8424\n",
      "Epoch 811/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.5546 - val_loss: 0.8425\n",
      "Epoch 812/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.5512 - val_loss: 0.8405\n",
      "Epoch 813/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.5538 - val_loss: 0.8353\n",
      "Epoch 814/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.5490 - val_loss: 0.8359\n",
      "Epoch 815/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.5541 - val_loss: 0.8331\n",
      "Epoch 816/1000\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.5513 - val_loss: 0.8382\n",
      "Epoch 817/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.5493 - val_loss: 0.8329\n",
      "Epoch 818/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.5455 - val_loss: 0.8358\n",
      "Epoch 819/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.5471 - val_loss: 0.8321\n",
      "Epoch 820/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.5417 - val_loss: 0.8326\n",
      "Epoch 821/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.5448 - val_loss: 0.8252\n",
      "Epoch 822/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.5434 - val_loss: 0.8235\n",
      "Epoch 823/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.5497 - val_loss: 0.8251\n",
      "Epoch 824/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.5392 - val_loss: 0.8229\n",
      "Epoch 825/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.5449 - val_loss: 0.8243\n",
      "Epoch 826/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.5361 - val_loss: 0.8194\n",
      "Epoch 827/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.5495 - val_loss: 0.8215\n",
      "Epoch 828/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.5367 - val_loss: 0.8229\n",
      "Epoch 829/1000\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.5519 - val_loss: 0.8156\n",
      "Epoch 830/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.5385 - val_loss: 0.8263\n",
      "Epoch 831/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.5465 - val_loss: 0.8087\n",
      "Epoch 832/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.5324 - val_loss: 0.8267\n",
      "Epoch 833/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.5445 - val_loss: 0.8072\n",
      "Epoch 834/1000\n",
      "160/160 [==============================] - 0s 296us/step - loss: 0.5452 - val_loss: 0.8258\n",
      "Epoch 835/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.5373 - val_loss: 0.8078\n",
      "Epoch 836/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.5308 - val_loss: 0.8152\n",
      "Epoch 837/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.5339 - val_loss: 0.8019\n",
      "Epoch 838/1000\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.5279 - val_loss: 0.8058\n",
      "Epoch 839/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.5304 - val_loss: 0.8026\n",
      "Epoch 840/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.5253 - val_loss: 0.7969\n",
      "Epoch 841/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.5200 - val_loss: 0.7947\n",
      "Epoch 842/1000\n",
      "160/160 [==============================] - 0s 263us/step - loss: 0.5274 - val_loss: 0.7934\n",
      "Epoch 843/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.5177 - val_loss: 0.7976\n",
      "Epoch 844/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.5225 - val_loss: 0.7903\n",
      "Epoch 845/1000\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.5177 - val_loss: 0.7911\n",
      "Epoch 846/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 208us/step - loss: 0.5263 - val_loss: 0.7869\n",
      "Epoch 847/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.5147 - val_loss: 0.7823\n",
      "Epoch 848/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.5157 - val_loss: 0.7813\n",
      "Epoch 849/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.5158 - val_loss: 0.7811\n",
      "Epoch 850/1000\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.5207 - val_loss: 0.7826\n",
      "Epoch 851/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.5190 - val_loss: 0.7849\n",
      "Epoch 852/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.5149 - val_loss: 0.7799\n",
      "Epoch 853/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.5096 - val_loss: 0.7780\n",
      "Epoch 854/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.5078 - val_loss: 0.7775\n",
      "Epoch 855/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.5067 - val_loss: 0.7782\n",
      "Epoch 856/1000\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.5056 - val_loss: 0.7761\n",
      "Epoch 857/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.5052 - val_loss: 0.7744\n",
      "Epoch 858/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.5062 - val_loss: 0.7722\n",
      "Epoch 859/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.5049 - val_loss: 0.7693\n",
      "Epoch 860/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.5064 - val_loss: 0.7721\n",
      "Epoch 861/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.5027 - val_loss: 0.7691\n",
      "Epoch 862/1000\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.5066 - val_loss: 0.7704\n",
      "Epoch 863/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.5019 - val_loss: 0.7656\n",
      "Epoch 864/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.5011 - val_loss: 0.7607\n",
      "Epoch 865/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.4976 - val_loss: 0.7608\n",
      "Epoch 866/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.4992 - val_loss: 0.7603\n",
      "Epoch 867/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.4971 - val_loss: 0.7586\n",
      "Epoch 868/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.5020 - val_loss: 0.7573\n",
      "Epoch 869/1000\n",
      "160/160 [==============================] - 0s 267us/step - loss: 0.4948 - val_loss: 0.7620\n",
      "Epoch 870/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.4985 - val_loss: 0.7558\n",
      "Epoch 871/1000\n",
      "160/160 [==============================] - 0s 273us/step - loss: 0.4972 - val_loss: 0.7594\n",
      "Epoch 872/1000\n",
      "160/160 [==============================] - 0s 248us/step - loss: 0.4993 - val_loss: 0.7541\n",
      "Epoch 873/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.4923 - val_loss: 0.7502\n",
      "Epoch 874/1000\n",
      "160/160 [==============================] - 0s 270us/step - loss: 0.4889 - val_loss: 0.7473\n",
      "Epoch 875/1000\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.4896 - val_loss: 0.7459\n",
      "Epoch 876/1000\n",
      "160/160 [==============================] - 0s 253us/step - loss: 0.4872 - val_loss: 0.7454\n",
      "Epoch 877/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.4868 - val_loss: 0.7464\n",
      "Epoch 878/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.4861 - val_loss: 0.7451\n",
      "Epoch 879/1000\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.4980 - val_loss: 0.7448\n",
      "Epoch 880/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.4827 - val_loss: 0.7464\n",
      "Epoch 881/1000\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.4885 - val_loss: 0.7438\n",
      "Epoch 882/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 0.4833 - val_loss: 0.7407\n",
      "Epoch 883/1000\n",
      "160/160 [==============================] - 0s 259us/step - loss: 0.4878 - val_loss: 0.7402\n",
      "Epoch 884/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.4810 - val_loss: 0.7402\n",
      "Epoch 885/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.4837 - val_loss: 0.7361\n",
      "Epoch 886/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 0.4781 - val_loss: 0.7386\n",
      "Epoch 887/1000\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.4806 - val_loss: 0.7344\n",
      "Epoch 888/1000\n",
      "160/160 [==============================] - 0s 264us/step - loss: 0.4854 - val_loss: 0.7345\n",
      "Epoch 889/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.4778 - val_loss: 0.7339\n",
      "Epoch 890/1000\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.4772 - val_loss: 0.7293\n",
      "Epoch 891/1000\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.4746 - val_loss: 0.7273\n",
      "Epoch 892/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.4763 - val_loss: 0.7265\n",
      "Epoch 893/1000\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.4732 - val_loss: 0.7238\n",
      "Epoch 894/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.4730 - val_loss: 0.7230\n",
      "Epoch 895/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 0.4790 - val_loss: 0.7209\n",
      "Epoch 896/1000\n",
      "160/160 [==============================] - 0s 257us/step - loss: 0.4697 - val_loss: 0.7209\n",
      "Epoch 897/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.4802 - val_loss: 0.7193\n",
      "Epoch 898/1000\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.4701 - val_loss: 0.7246\n",
      "Epoch 899/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.4741 - val_loss: 0.7157\n",
      "Epoch 900/1000\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.4707 - val_loss: 0.7229\n",
      "Epoch 901/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.4703 - val_loss: 0.7168\n",
      "Epoch 902/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.4684 - val_loss: 0.7166\n",
      "Epoch 903/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.4712 - val_loss: 0.7134\n",
      "Epoch 904/1000\n",
      "160/160 [==============================] - 0s 275us/step - loss: 0.4644 - val_loss: 0.7109\n",
      "Epoch 905/1000\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.4647 - val_loss: 0.7103\n",
      "Epoch 906/1000\n",
      "160/160 [==============================] - 0s 255us/step - loss: 0.4638 - val_loss: 0.7109\n",
      "Epoch 907/1000\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.4623 - val_loss: 0.7078\n",
      "Epoch 908/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.4651 - val_loss: 0.7073\n",
      "Epoch 909/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.4722 - val_loss: 0.7085\n",
      "Epoch 910/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.4625 - val_loss: 0.7085\n",
      "Epoch 911/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.4649 - val_loss: 0.7125\n",
      "Epoch 912/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.4612 - val_loss: 0.7067\n",
      "Epoch 913/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.4630 - val_loss: 0.7030\n",
      "Epoch 914/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.4651 - val_loss: 0.7034\n",
      "Epoch 915/1000\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.4564 - val_loss: 0.7012\n",
      "Epoch 916/1000\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.467 - 0s 213us/step - loss: 0.4579 - val_loss: 0.7001\n",
      "Epoch 917/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4720 - val_loss: 0.6975\n",
      "Epoch 918/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.4516 - val_loss: 0.7020\n",
      "Epoch 919/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.4605 - val_loss: 0.6919\n",
      "Epoch 920/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.4518 - val_loss: 0.6946\n",
      "Epoch 921/1000\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4569 - val_loss: 0.6918\n",
      "Epoch 922/1000\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.4543 - val_loss: 0.6953\n",
      "Epoch 923/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.4522 - val_loss: 0.6902\n",
      "Epoch 924/1000\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.4517 - val_loss: 0.6918\n",
      "Epoch 925/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.4594 - val_loss: 0.6899\n",
      "Epoch 926/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.4556 - val_loss: 0.6863\n",
      "Epoch 927/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.4516 - val_loss: 0.6859\n",
      "Epoch 928/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.4483 - val_loss: 0.6832\n",
      "Epoch 929/1000\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.4467 - val_loss: 0.6832\n",
      "Epoch 930/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.4486 - val_loss: 0.6829\n",
      "Epoch 931/1000\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.4448 - val_loss: 0.6832\n",
      "Epoch 932/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.4452 - val_loss: 0.6808\n",
      "Epoch 933/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.4452 - val_loss: 0.6803\n",
      "Epoch 934/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.4427 - val_loss: 0.6788\n",
      "Epoch 935/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.4424 - val_loss: 0.6773\n",
      "Epoch 936/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.4419 - val_loss: 0.6769\n",
      "Epoch 937/1000\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.4405 - val_loss: 0.6769\n",
      "Epoch 938/1000\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.4404 - val_loss: 0.6733\n",
      "Epoch 939/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.4400 - val_loss: 0.6727\n",
      "Epoch 940/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.4439 - val_loss: 0.6730\n",
      "Epoch 941/1000\n",
      "160/160 [==============================] - 0s 300us/step - loss: 0.4430 - val_loss: 0.6720\n",
      "Epoch 942/1000\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.4436 - val_loss: 0.6695\n",
      "Epoch 943/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.4371 - val_loss: 0.6687\n",
      "Epoch 944/1000\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.4377 - val_loss: 0.6678\n",
      "Epoch 945/1000\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.4361 - val_loss: 0.6683\n",
      "Epoch 946/1000\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.4368 - val_loss: 0.6668\n",
      "Epoch 947/1000\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.4363 - val_loss: 0.6656\n",
      "Epoch 948/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.4330 - val_loss: 0.6654\n",
      "Epoch 949/1000\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.4322 - val_loss: 0.6633\n",
      "Epoch 950/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.4397 - val_loss: 0.6624\n",
      "Epoch 951/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4363 - val_loss: 0.6640\n",
      "Epoch 952/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.4331 - val_loss: 0.6624\n",
      "Epoch 953/1000\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.4359 - val_loss: 0.6597\n",
      "Epoch 954/1000\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.4312 - val_loss: 0.6611\n",
      "Epoch 955/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.4320 - val_loss: 0.6576\n",
      "Epoch 956/1000\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.4337 - val_loss: 0.6584\n",
      "Epoch 957/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4389 - val_loss: 0.6578\n",
      "Epoch 958/1000\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.4307 - val_loss: 0.6546\n",
      "Epoch 959/1000\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.4318 - val_loss: 0.6527\n",
      "Epoch 960/1000\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.4282 - val_loss: 0.6524\n",
      "Epoch 961/1000\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.4265 - val_loss: 0.6520\n",
      "Epoch 962/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.4244 - val_loss: 0.6524\n",
      "Epoch 963/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.4264 - val_loss: 0.6504\n",
      "Epoch 964/1000\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.4247 - val_loss: 0.6501\n",
      "Epoch 965/1000\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.4313 - val_loss: 0.6488\n",
      "Epoch 966/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.4259 - val_loss: 0.6552\n",
      "Epoch 967/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.4260 - val_loss: 0.6449\n",
      "Epoch 968/1000\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.4254 - val_loss: 0.6437\n",
      "Epoch 969/1000\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.4265 - val_loss: 0.6446\n",
      "Epoch 970/1000\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.4267 - val_loss: 0.6412\n",
      "Epoch 971/1000\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.4215 - val_loss: 0.6418\n",
      "Epoch 972/1000\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.4193 - val_loss: 0.6425\n",
      "Epoch 973/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4187 - val_loss: 0.6425\n",
      "Epoch 974/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.4179 - val_loss: 0.6410\n",
      "Epoch 975/1000\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.4202 - val_loss: 0.6382\n",
      "Epoch 976/1000\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.4217 - val_loss: 0.6404\n",
      "Epoch 977/1000\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.4333 - val_loss: 0.6374\n",
      "Epoch 978/1000\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.4174 - val_loss: 0.6366\n",
      "Epoch 979/1000\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.4196 - val_loss: 0.6352\n",
      "Epoch 980/1000\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.4236 - val_loss: 0.6376\n",
      "Epoch 981/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.4167 - val_loss: 0.6373\n",
      "Epoch 982/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.4162 - val_loss: 0.6331\n",
      "Epoch 983/1000\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.4119 - val_loss: 0.6320\n",
      "Epoch 984/1000\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.4216 - val_loss: 0.6285\n",
      "Epoch 985/1000\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.4265 - val_loss: 0.6400\n",
      "Epoch 986/1000\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.4183 - val_loss: 0.6329\n",
      "Epoch 987/1000\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.4296 - val_loss: 0.6404\n",
      "Epoch 988/1000\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.4147 - val_loss: 0.6442\n",
      "Epoch 989/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.4295 - val_loss: 0.6260\n",
      "Epoch 990/1000\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.4073 - val_loss: 0.6444\n",
      "Epoch 991/1000\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.4245 - val_loss: 0.6260\n",
      "Epoch 992/1000\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.4076 - val_loss: 0.6324\n",
      "Epoch 993/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4125 - val_loss: 0.6238\n",
      "Epoch 994/1000\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.4190 - val_loss: 0.6279\n",
      "Epoch 995/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.4190 - val_loss: 0.6312\n",
      "Epoch 996/1000\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.4129 - val_loss: 0.6209\n",
      "Epoch 997/1000\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.4050 - val_loss: 0.6252\n",
      "Epoch 998/1000\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.4136 - val_loss: 0.6181\n",
      "Epoch 999/1000\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.4370 - val_loss: 0.6294\n",
      "Epoch 1000/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/160 [==============>...............] - ETA: 0s - loss: 0.5038\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "160/160 [==============================] - 0s 195us/step - loss: 0.4083 - val_loss: 0.6377\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=80,epochs=1000,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从输出的数据可以看出：一开始，loss的值非常大，而随着训练不断的进行，loss在逐渐减小。\n",
    "\n",
    "将history中的数据通过matplotlib绘图表现出来，能够更加直观地看出loss的变化规律。因为在jupyter上调试，代码中加入%matplotlib inline命令，让图片输出在网页中。下面的图中，将训练过程中的训练集loss以及验证集loss全部输出，并打印出测试集的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 140us/step\n",
      "test_loss: 0.423468679189682\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#将图片内嵌在交互窗口，而不是弹出一个图片窗口\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1000),history.history['loss'])\n",
    "plt.plot(range(1000),history.history['val_loss'])\n",
    "print(\"test_loss:\",model.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何评价一个模型训练是否成功？首先，训练过程中训练集loss要下降到一个较小的值，表示模型收敛较好，没有欠拟合；其次，测试集loss最后与训练集loss要尽可能相似，差距越小越好小，说明该模型没有过拟合。\n",
    "\n",
    "当一个神经网络模型成功训练出来后，便可以使用该模型进行预测了。通过pandas的DataFrame方法构造x_input，并使用模型的`predict`方法进行预测。这里的数据是根据`Advertising.csv`的前三条略加修改的，可以看看这个模型输出的结果与真实结果（sales列）是否一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.279892],\n",
       "       [10.489032],\n",
       "       [ 9.173268]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用字典生成的DataFrame，需要指定一下列的次序\n",
    "d={'TV':[230,44,17],'radio':[37,39,45],'newspaper':[69,45,69]}\n",
    "x_input=pd.DataFrame(d,columns=['TV','radio','newspaper'])\n",
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230</td>\n",
       "      <td>37</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>45</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TV  radio  newspaper\n",
       "0  230     37         69\n",
       "1   44     39         45\n",
       "2   17     45         69"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用输出来的结果和前三条数据比较（原来数据中的`sales`分别为：22，10，9），看起来效果还是不错的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.保存模型\n",
    "\n",
    "训练出来的模型，可以保存。下次使用的时候载入，还可以继续训练。一般保存为h5格式，需要先安装h5py。\n",
    "\n",
    "命令如下：pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/1-model-vv.h5')   # HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.应用模型\n",
    "\n",
    "使用keras.models的load_model语句载入模型，就可以直接用这个模型来做预测了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "model = load_model('./model/1-model-vv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码将在`data`文件夹中生成一个名为`test.csv`的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/test.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/test.csv\n",
    "TV,radio,newspaper\n",
    "230.0,37.0,69.0\n",
    "44,39,45\n",
    "17,45,69\n",
    "283.1,42.1,66.1\n",
    "232.1,8.6,8.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.279892],\n",
       "       [10.489032],\n",
       "       [ 9.173268],\n",
       "       [24.312817],\n",
       "       [13.093816]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据，并且输出预测结果\n",
    "x_input=pd.read_csv('./data/test.csv')\n",
    "model.predict(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：**可以导入模型后，继续训练，直到loss不会继续变小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
